From: root <root@localhost.localdomain>
Date: Thu, 21 May 2020 08:49:27 -0700
Subject: [PATCH] with MILAN Support .

(UPSTREAM) commit 5738891229a25e9e678122a843cbf0466a456d0c
Author: Kim Phillips <kim.phillips@amd.com>
Date:   Thu Nov 14 12:37:20 2019 -0600

    perf/x86/amd: Add support for Large Increment per Cycle Events

    Description of hardware operation
    ---------------------------------

    The core AMD PMU has a 4-bit wide per-cycle increment for each
    performance monitor counter.  That works for most events, but
    now with AMD Family 17h and above processors, some events can
    occur more than 15 times in a cycle.  Those events are called
    "Large Increment per Cycle" events. In order to count these
    events, two adjacent h/w PMCs get their count signals merged
    to form 8 bits per cycle total.  In addition, the PERF_CTR count
    registers are merged to be able to count up to 64 bits.

    Normally, events like instructions retired, get programmed on a single
    counter like so:

    PERF_CTL0 (MSR 0xc0010200) 0x000000000053ff0c # event 0x0c, umask 0xff
    PERF_CTR0 (MSR 0xc0010201) 0x0000800000000001 # r/w 48-bit count

    The next counter at MSRs 0xc0010202-3 remains unused, or can be used
    independently to count something else.

    When counting Large Increment per Cycle events, such as FLOPs,
    however, we now have to reserve the next counter and program the
    PERF_CTL (config) register with the Merge event (0xFFF), like so:

    PERF_CTL0 (msr 0xc0010200) 0x000000000053ff03 # FLOPs event, umask 0xff
    PERF_CTR0 (msr 0xc0010201) 0x0000800000000001 # rd 64-bit cnt, wr lo 48b
    PERF_CTL1 (msr 0xc0010202) 0x0000000f004000ff # Merge event, enable bit
    PERF_CTR1 (msr 0xc0010203) 0x0000000000000000 # wr hi 16-bits count

    The count is widened from the normal 48-bits to 64 bits by having the
    second counter carry the higher 16 bits of the count in its lower 16
    bits of its counter register.

    The odd counter, e.g., PERF_CTL1, is programmed with the enabled Merge
    event before the even counter, PERF_CTL0.

    The Large Increment feature is available starting with Family 17h.
    For more details, search any Family 17h PPR for the "Large Increment
    per Cycle Events" section, e.g., section 2.1.15.3 on p. 173 in this
    version:

    https://www.amd.com/system/files/TechDocs/56176_ppr_Family_17h_Model_71h_B0_pub_Rev_3.06.zip

    Description of software operation
    ---------------------------------

    The following steps are taken in order to support reserving and
    enabling the extra counter for Large Increment per Cycle events:

    1. In the main x86 scheduler, we reduce the number of available
    counters by the number of Large Increment per Cycle events being
    scheduled, tracked by a new cpuc variable 'n_pair' and a new
    amd_put_event_constraints_f17h().  This improves the counter
    scheduler success rate.

    2. In perf_assign_events(), if a counter is assigned to a Large
    Increment event, we increment the current counter variable, so the
    counter used for the Merge event is removed from assignment
    consideration by upcoming event assignments.

    3. In find_counter(), if a counter has been found for the Large
    Increment event, we set the next counter as used, to prevent other
    events from using it.

    4. We perform steps 2 & 3 also in the x86 scheduler fastpath, i.e.,
    we add Merge event accounting to the existing used_mask logic.

    5. Finally, we add on the programming of Merge event to the
    neighbouring PMC counters in the counter enable/disable{_all}
    code paths.

    Currently, software does not support a single PMU with mixed 48- and
    64-bit counting, so Large increment event counts are limited to 48
    bits.  In set_period, we zero-out the upper 16 bits of the count, so
    the hardware doesn't copy them to the even counter's higher bits.

    Simple invocation example showing counting 8 FLOPs per 256-bit/%ymm
    vaddps instruction executed in a loop 100 million times:

    perf stat -e cpu/fp_ret_sse_avx_ops.all/,cpu/instructions/ <workload>

    Performance counter stats for '<workload>':

           800,000,000      cpu/fp_ret_sse_avx_ops.all/u
           300,042,101      cpu/instructions/u

    Prior to this patch, the reported SSE/AVX FLOPs retired count would
    be wrong.

    [peterz: lots of renames and edits to the code]

Signed-off-by: Kim Phillips <kim.phillips@amd.com>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>



(UPSTREAM) commit 471af006a747f1c535c8a8c6c0973c320fe01b22
Author: Kim Phillips <kim.phillips@amd.com>
Date:   Thu Nov 14 12:37:19 2019 -0600

    perf/x86/amd: Constrain Large Increment per Cycle events

    AMD Family 17h processors and above gain support for Large Increment
    per Cycle events.  Unfortunately there is no CPUID or equivalent bit
    that indicates whether the feature exists or not, so we continue to
    determine eligibility based on a CPU family number comparison.

    For Large Increment per Cycle events, we add a f17h-and-compatibles
    get_event_constraints_f17h() that returns an even counter bitmask:
    Large Increment per Cycle events can only be placed on PMCs 0, 2,
    and 4 out of the currently available 0-5.  The only currently
    public event that requires this feature to report valid counts
    is PMCx003 "Retired SSE/AVX Operations".

    Note that the CPU family logic in amd_core_pmu_init() is changed
    so as to be able to selectively add initialization for features
    available in ranges of backward-compatible CPU families.  This
    Large Increment per Cycle feature is expected to be retained
    in future families.

    A side-effect of assigning a new get_constraints function for f17h
    disables calling the old (prior to f15h) amd_get_event_constraints
    implementation left enabled by commit e40ed1542dd7 ("perf/x86: Add perf
    support for AMD family-17h processors"), which is no longer
    necessary since those North Bridge event codes are obsoleted.

    Also fix a spelling mistake whilst in the area (calulating ->
    calculating).

    Fixes: e40ed1542dd7 ("perf/x86: Add perf support for AMD family-17h processors")
Signed-off-by: Kim Phillips <kim.phillips@amd.com>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191114183720.19887-2-kim.phillips@amd.com



---
 arch/x86/events/amd/core.c   | 158 ++++++++++++++++++++++++-----------
 arch/x86/events/core.c       |  98 +++++++++++++++-------
 arch/x86/events/perf_event.h |  44 +++++++---
 3 files changed, 209 insertions(+), 91 deletions(-)

diff --git a/arch/x86/events/amd/core.c b/arch/x86/events/amd/core.c
index 8f56747f..593c3418 100644
--- a/arch/x86/events/amd/core.c
+++ b/arch/x86/events/amd/core.c
@@ -4,12 +4,18 @@
 #include <linux/init.h>
 #include <linux/slab.h>
 #include <linux/delay.h>
+#include <linux/jiffies.h>
 #include <asm/apicdef.h>
 #include <asm/nmi.h>
 
 #include "../perf_event.h"
 
 static DEFINE_PER_CPU(unsigned int, perf_nmi_counter);
+static unsigned long perf_nmi_window;
+
+/* AMD Event 0xFFF: Merge.  Used with Large Increment per Cycle events */
+#define AMD_MERGE_EVENT ((0xFULL << 32) | 0xFFULL)
+#define AMD_MERGE_EVENT_ENABLE (AMD_MERGE_EVENT | ARCH_PERFMON_EVENTSEL_ENABLE)
 
 static __initconst const u64 amd_hw_cache_event_ids
 				[PERF_COUNT_HW_CACHE_MAX]
@@ -177,6 +183,30 @@ static inline int amd_pmu_addr_offset(int index, bool eventsel)
 	return offset;
 }
 
+/*
+ * AMD64 events are detected based on their event codes.
+ */
+static inline unsigned int amd_get_event_code(struct hw_perf_event *hwc)
+{
+	return ((hwc->config >> 24) & 0x0f00) | (hwc->config & 0x00ff);
+}
+
+static inline bool amd_is_pair_event_code(struct hw_perf_event *hwc)
+{
+	if (!(x86_pmu.flags & PMU_FL_PAIR))
+		return false;
+
+	switch (amd_get_event_code(hwc)) {
+	case 0x003:     return true;    /* Retired SSE/AVX FLOPs */
+	default:        return false;
+	}
+}
+
+static inline int amd_is_nb_event(struct hw_perf_event *hwc)
+{
+	return (hwc->config & 0xe0) == 0xe0;
+}
+
 static int amd_core_hw_config(struct perf_event *event)
 {
 	if (event->attr.exclude_host && event->attr.exclude_guest)
@@ -192,20 +222,10 @@ static int amd_core_hw_config(struct perf_event *event)
 	else if (event->attr.exclude_guest)
 		event->hw.config |= AMD64_EVENTSEL_HOSTONLY;
 
-	return 0;
-}
-
-/*
- * AMD64 events are detected based on their event codes.
- */
-static inline unsigned int amd_get_event_code(struct hw_perf_event *hwc)
-{
-	return ((hwc->config >> 24) & 0x0f00) | (hwc->config & 0x00ff);
-}
+	if ((x86_pmu.flags & PMU_FL_PAIR) && amd_is_pair_event_code(&event->hw))
+		event->hw.flags |= PERF_X86_EVENT_PAIR;
 
-static inline int amd_is_nb_event(struct hw_perf_event *hwc)
-{
-	return (hwc->config & 0xe0) == 0xe0;
+	return 0;
 }
 
 static inline int amd_has_nb(struct cpu_hw_events *cpuc)
@@ -546,7 +566,7 @@ static int amd_pmu_handle_irq(struct pt_regs *regs)
 	 */
 	if (handled) {
 		this_cpu_write(perf_nmi_counter,
-			       min_t(unsigned int, 2, active));
+			       jiffies + perf_nmi_window);
 
 		return handled;
 	}
@@ -579,6 +599,16 @@ static void amd_put_event_constraints(struct cpu_hw_events *cpuc,
 		__amd_put_nb_event_constraints(cpuc, event);
 }
 
+static void amd_put_event_constraints_f17h(struct cpu_hw_events *cpuc,
+					  struct perf_event *event)
+{
+       struct hw_perf_event *hwc = &event->hw;
+
+       if (is_counter_pair(hwc))
+	       --cpuc->n_pair;
+}
+
+
 PMU_FORMAT_ATTR(event,	"config:0-7,32-35");
 PMU_FORMAT_ATTR(umask,	"config:8-15"	);
 PMU_FORMAT_ATTR(edge,	"config:18"	);
@@ -741,6 +771,20 @@ amd_get_event_constraints_f15h(struct cpu_hw_events *cpuc, int idx,
 	}
 }
 
+static struct event_constraint pair_constraint;
+
+static struct event_constraint *
+amd_get_event_constraints_f17h(struct cpu_hw_events *cpuc, int idx,
+			       struct perf_event *event)
+{
+	struct hw_perf_event *hwc = &event->hw;
+
+	if (amd_is_pair_event_code(hwc))
+		return &pair_constraint;
+
+	return &unconstrained;
+}
+
 static ssize_t amd_event_sysfs_show(char *page, u64 config)
 {
 	u64 event = (config & ARCH_PERFMON_EVENTSEL_EVENT) |
@@ -784,42 +828,56 @@ static __initconst const struct x86_pmu amd_pmu = {
 
 static int __init amd_core_pmu_init(void)
 {
-	if (!cpu_has_perfctr_core)
-		return 0;
-
-	switch (boot_cpu_data.x86) {
-	case 0x15:
-		pr_cont("Fam15h ");
-		x86_pmu.get_event_constraints = amd_get_event_constraints_f15h;
-		break;
-	case 0x17:
-		pr_cont("Fam17h ");
-		/*
-		 * In family 17h, there are no event constraints in the PMC hardware.
-		 * We fallback to using default amd_get_event_constraints.
-		 */
-		break;
-	default:
-		pr_err("core perfctr but no constraints; unknown hardware!\n");
-		return -ENODEV;
-	}
-
-	/*
-	 * If core performance counter extensions exists, we must use
-	 * MSR_F15H_PERF_CTL/MSR_F15H_PERF_CTR msrs. See also
-	 * amd_pmu_addr_offset().
-	 */
-	x86_pmu.eventsel	= MSR_F15H_PERF_CTL;
-	x86_pmu.perfctr		= MSR_F15H_PERF_CTR;
-	x86_pmu.num_counters	= AMD64_NUM_COUNTERS_CORE;
-	/*
-	 * AMD Core perfctr has separate MSRs for the NB events, see
-	 * the amd/uncore.c driver.
-	 */
-	x86_pmu.amd_nb_constraints = 0;
-
-	pr_cont("core perfctr, ");
-	return 0;
+	u64 even_ctr_mask = 0ULL;
+	int i;
+
+	if (!boot_cpu_has(X86_FEATURE_PERFCTR_CORE))
+		return 0;
+
+	/* Avoid calculating the value each time in the NMI handler */
+	perf_nmi_window = msecs_to_jiffies(100);
+
+	/*
+	 * If core performance counter extensions exists, we must use
+	 * MSR_F15H_PERF_CTL/MSR_F15H_PERF_CTR msrs. See also
+	 * amd_pmu_addr_offset().
+	 */
+	x86_pmu.eventsel        = MSR_F15H_PERF_CTL;
+	x86_pmu.perfctr         = MSR_F15H_PERF_CTR;
+	x86_pmu.num_counters    = AMD64_NUM_COUNTERS_CORE;
+	/*
+	 * AMD Core perfctr has separate MSRs for the NB events, see
+	 * the amd/uncore.c driver.
+	 */
+	x86_pmu.amd_nb_constraints = 0;
+
+	if (boot_cpu_data.x86 == 0x15) {
+		pr_cont("Fam15h ");
+		x86_pmu.get_event_constraints = amd_get_event_constraints_f15h;
+	}
+	if (boot_cpu_data.x86 >= 0x17) {
+		pr_cont("Fam17h+ ");
+		/*
+		 * Family 17h and compatibles have constraints for Large
+		 * Increment per Cycle events: they may only be assigned an
+		 * even numbered counter that has a consecutive adjacent odd
+		 * numbered counter following it.
+		 */
+		for (i = 0; i < x86_pmu.num_counters - 1; i += 2)
+			even_ctr_mask |= 1 << i;
+
+		pair_constraint = (struct event_constraint)
+				    __EVENT_CONSTRAINT(0, even_ctr_mask, 0,
+				    x86_pmu.num_counters / 2, 0,
+				    PERF_X86_EVENT_PAIR);
+		x86_pmu.get_event_constraints = amd_get_event_constraints_f17h;
+		x86_pmu.put_event_constraints = amd_put_event_constraints_f17h;
+		x86_pmu.perf_ctr_pair_en = AMD_MERGE_EVENT_ENABLE;
+		x86_pmu.flags |= PMU_FL_PAIR;
+	}
+
+	pr_cont("core perfctr, ");
+	return 0;
 }
 
 __init int amd_pmu_init(void)
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 878ca146..bd29e8e4 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -588,6 +588,7 @@ void x86_pmu_disable_all(void)
 	int idx;
 
 	for (idx = 0; idx < x86_pmu.num_counters; idx++) {
+		struct hw_perf_event *hwc = &cpuc->events[idx]->hw;
 		u64 val;
 
 		if (!test_bit(idx, cpuc->active_mask))
@@ -597,6 +598,8 @@ void x86_pmu_disable_all(void)
 			continue;
 		val &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
 		wrmsrl(x86_pmu_config_addr(idx), val);
+		if (is_counter_pair(hwc))
+			wrmsrl(x86_pmu_config_addr(idx + 1), 0);
 	}
 }
 
@@ -665,7 +668,7 @@ struct sched_state {
 	int	counter;	/* counter index */
 	int	unassigned;	/* number of events to be assigned left */
 	int	nr_gp;		/* number of GP counters used */
-	unsigned long used[BITS_TO_LONGS(X86_PMC_IDX_MAX)];
+	u64     used;
 };
 
 /* Total max is X86_PMC_IDX_MAX, but we are O(n!) limited */
@@ -722,8 +725,12 @@ static bool perf_sched_restore_state(struct perf_sched *sched)
 	sched->saved_states--;
 	sched->state = sched->saved[sched->saved_states];
 
-	/* continue with next counter: */
-	clear_bit(sched->state.counter++, sched->state.used);
+	/* this assignment didn't work out */
+	/* XXX broken vs EVENT_PAIR */
+	sched->state.used &= ~BIT_ULL(sched->state.counter);
+
+	/* try the next one */
+	sched->state.counter++;
 
 	return true;
 }
@@ -748,22 +755,29 @@ static bool __perf_sched_find_counter(struct perf_sched *sched)
 	if (c->idxmsk64 & (~0ULL << INTEL_PMC_IDX_FIXED)) {
 		idx = INTEL_PMC_IDX_FIXED;
 		for_each_set_bit_from(idx, c->idxmsk, X86_PMC_IDX_MAX) {
-			if (!__test_and_set_bit(idx, sched->state.used))
+			if (!__test_and_set_bit(idx, (volatile unsigned long *) &sched->state.used))
 				goto done;
 		}
 	}
 
 	/* Grab the first unused counter starting with idx */
-	idx = sched->state.counter;
-	for_each_set_bit_from(idx, c->idxmsk, INTEL_PMC_IDX_FIXED) {
-		if (!__test_and_set_bit(idx, sched->state.used)) {
-			if (sched->state.nr_gp++ >= sched->max_gp)
-				return false;
+	idx = sched->state.counter;
+	for_each_set_bit_from(idx, c->idxmsk, INTEL_PMC_IDX_FIXED) {
+		u64 mask = BIT_ULL(idx);
 
-			goto done;
-		}
-	}
+		if (c->flags & PERF_X86_EVENT_PAIR)
+			mask |= mask << 1;
 
+		if (sched->state.used & mask)
+			continue;
+
+		if (sched->state.nr_gp++ >= sched->max_gp)
+			return false;
+
+		sched->state.used |= mask;
+		goto done;
+	}
+
 	return false;
 
 done:
@@ -838,12 +852,11 @@ EXPORT_SYMBOL_GPL(perf_assign_events);
 int x86_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)
 {
 	struct event_constraint *c;
-	unsigned long used_mask[BITS_TO_LONGS(X86_PMC_IDX_MAX)];
 	struct perf_event *e;
 	int i, wmin, wmax, unsched = 0;
 	struct hw_perf_event *hwc;
 
-	bitmap_zero(used_mask, X86_PMC_IDX_MAX);
+	u64 used_mask = 0;
 
 	if (x86_pmu.start_scheduling)
 		x86_pmu.start_scheduling(cpuc);
@@ -861,25 +874,32 @@ int x86_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)
 	 * fastpath, try to reuse previous register
 	 */
 	for (i = 0; i < n; i++) {
-		hwc = &cpuc->event_list[i]->hw;
-		c = cpuc->event_constraint[i];
+		u64 mask;
 
-		/* never assigned */
-		if (hwc->idx == -1)
-			break;
+		hwc = &cpuc->event_list[i]->hw;
+		c = cpuc->event_constraint[i];
 
-		/* constraint still honored */
-		if (!test_bit(hwc->idx, c->idxmsk))
-			break;
+		/* never assigned */
+		if (hwc->idx == -1)
+			break;
 
-		/* not already used */
-		if (test_bit(hwc->idx, used_mask))
-			break;
+		/* constraint still honored */
+		if (!test_bit(hwc->idx, c->idxmsk))
+			break;
 
-		__set_bit(hwc->idx, used_mask);
-		if (assign)
-			assign[i] = hwc->idx;
-	}
+		mask = BIT_ULL(hwc->idx);
+		if (is_counter_pair(hwc))
+			mask |= mask << 1;
+
+		/* not already used */
+		if (used_mask & mask)
+			break;
+
+		used_mask |= mask;
+
+		if (assign)
+			assign[i] = hwc->idx;
+	}
 
 	/* slow path */
 	if (i != n) {
@@ -898,6 +918,15 @@ int x86_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)
 		if (is_ht_workaround_enabled() && !cpuc->is_fake &&
 		    READ_ONCE(cpuc->excl_cntrs->exclusive_present))
 			gpmax /= 2;
+
+		 /*
+		 * Reduce the amount of available counters to allow fitting
+		 * the extra Merge events needed by large increment events.
+		 */
+		if (x86_pmu.flags & PMU_FL_PAIR) {
+			gpmax = x86_pmu.num_counters - cpuc->n_pair;
+			WARN_ON(gpmax <= 0);
+		}
 
 		unsched = perf_assign_events(cpuc->event_constraint, n, wmin,
 					     wmax, gpmax, assign);
@@ -963,6 +992,8 @@ static int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader,
 			return -EINVAL;
 		cpuc->event_list[n] = leader;
 		n++;
+		if (is_counter_pair(&leader->hw))
+			cpuc->n_pair++;
 	}
 	if (!dogrp)
 		return n;
@@ -977,6 +1008,8 @@ static int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader,
 
 		cpuc->event_list[n] = event;
 		n++;
+		if (is_counter_pair(&event->hw))
+			cpuc->n_pair++;
 	}
 	return n;
 }
@@ -1141,6 +1174,13 @@ int x86_perf_event_set_period(struct perf_event *event)
 
 	wrmsrl(hwc->event_base, (u64)(-left) & x86_pmu.cntval_mask);
 
+	 /*
+	 * Clear the Merge event counter's upper 16 bits since
+	 * we currently declare a 48-bit counter width
+	 */
+	if (is_counter_pair(hwc))
+		wrmsrl(x86_pmu_event_addr(idx + 1), 0);
+
 	/*
 	 * Due to erratum on certan cpu we need
 	 * a second write to be sure the register
diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h
index ab3b4e8e..75ebbe50 100644
--- a/arch/x86/events/perf_event.h
+++ b/arch/x86/events/perf_event.h
@@ -56,18 +56,19 @@ struct event_constraint {
 /*
  * struct hw_perf_event.flags flags
  */
-#define PERF_X86_EVENT_PEBS_LDLAT	0x1 /* ld+ldlat data address sampling */
-#define PERF_X86_EVENT_PEBS_ST		0x2 /* st data address sampling */
-#define PERF_X86_EVENT_PEBS_ST_HSW	0x4 /* haswell style datala, store */
-#define PERF_X86_EVENT_COMMITTED	0x8 /* event passed commit_txn */
-#define PERF_X86_EVENT_PEBS_LD_HSW	0x10 /* haswell style datala, load */
-#define PERF_X86_EVENT_PEBS_NA_HSW	0x20 /* haswell style datala, unknown */
-#define PERF_X86_EVENT_EXCL		0x40 /* HT exclusivity on counter */
-#define PERF_X86_EVENT_DYNAMIC		0x80 /* dynamic alloc'd constraint */
-#define PERF_X86_EVENT_EXCL_ACCT	0x0200 /* accounted EXCL event */
-#define PERF_X86_EVENT_AUTO_RELOAD	0x0400 /* use PEBS auto-reload */
-#define PERF_X86_EVENT_FREERUNNING	0x0800 /* use freerunning PEBS */
-
+#define PERF_X86_EVENT_PEBS_LDLAT       0x0001 /* ld+ldlat data address sampling */
+#define PERF_X86_EVENT_PEBS_ST          0x0002 /* st data address sampling */
+#define PERF_X86_EVENT_PEBS_ST_HSW      0x0004 /* haswell style datala, store */
+#define PERF_X86_EVENT_COMMITTED        0x0008 /* haswell style datala, load */
+#define PERF_X86_EVENT_PEBS_LD_HSW      0x10 /* haswell style datala, load */
+#define PERF_X86_EVENT_PEBS_NA_HSW      0x20 /* haswell style datala, unknown */
+#define PERF_X86_EVENT_EXCL             0x40 /* HT exclusivity on counter */
+#define PERF_X86_EVENT_DYNAMIC          0x80 /* dynamic alloc'd constraint */
+#define PERF_X86_EVENT_EXCL_ACCT        0x0100 /* accounted EXCL event */
+#define PERF_X86_EVENT_AUTO_RELOAD      0x0200 /* use PEBS auto-reload */
+#define PERF_X86_EVENT_LARGE_PEBS       0x0400 /* use large PEBS */
+#define PERF_X86_EVENT_FREERUNNING      0x0800 /* use freerunning PEBS */
+#define PERF_X86_EVENT_PAIR             0x1000 /* Large Increment per Cycle */
 
 struct amd_nb {
 	int nb_id;  /* NorthBridge id */
@@ -265,6 +266,7 @@ struct cpu_hw_events {
 	struct amd_nb			*amd_nb;
 	/* Inverted mask of bits to clear in the perf_ctr ctrl registers */
 	u64				perf_ctr_virt_mask;
+	int                             n_pair; /* Large increment events */
 
 	void				*kfree_on_online[X86_PERF_KFREE_MAX];
 };
@@ -652,6 +654,7 @@ struct x86_pmu {
 	 * AMD bits
 	 */
 	unsigned int	amd_nb_constraints : 1;
+	u64             perf_ctr_pair_en;
 
 	/*
 	 * Extra registers for events
@@ -691,6 +694,7 @@ do {									\
 #define PMU_FL_EXCL_ENABLED	0x8 /* exclusive counter active */
 #define PMU_FL_PEBS_ALL		0x10 /* all events are valid PEBS events */
 #define PMU_FL_TFA		0x20 /* deal with TSX force abort */
+#define PMU_FL_PAIR             0x40 /* merge counters for large incr. events */
 
 #define EVENT_VAR(_id)  event_attr_##_id
 #define EVENT_PTR(_id) &event_attr_##_id.attr.attr
@@ -777,6 +781,11 @@ int x86_pmu_hw_config(struct perf_event *event);
 
 void x86_pmu_disable_all(void);
 
+static inline bool is_counter_pair(struct hw_perf_event *hwc)
+{
+       return hwc->flags & PERF_X86_EVENT_PAIR;
+}
+
 static inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,
 					  u64 enable_mask)
 {
@@ -784,6 +793,13 @@ static inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,
 
 	if (hwc->extra_reg.reg)
 		wrmsrl(hwc->extra_reg.reg, hwc->extra_reg.config);
+
+	/*
+	 * Add enabled Merge event on next counter
+	 * if large increment event being enabled on this counter
+	 */
+	if (is_counter_pair(hwc))
+		wrmsrl(x86_pmu_config_addr(hwc->idx + 1), x86_pmu.perf_ctr_pair_en);
 	wrmsrl(hwc->config_base, (hwc->config | enable_mask) & ~disable_mask);
 }
 
@@ -800,6 +816,10 @@ static inline void x86_pmu_disable_event(struct perf_event *event)
 	struct hw_perf_event *hwc = &event->hw;
 
 	wrmsrl(hwc->config_base, hwc->config);
+
+	if (is_counter_pair(hwc))
+		wrmsrl(x86_pmu_config_addr(hwc->idx + 1), 0);
+
 }
 
 void x86_pmu_enable_event(struct perf_event *event);
-- 
2.17.0

